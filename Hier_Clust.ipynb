{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hierarchical Clustering Project\n",
        "\n",
        "In this notebook, we will explore Hierarchical Clustering using a dataset from the [Datasets repository](https://github.com/MainakRepositor/Datasets). We will provide an overview of hierarchical clustering, its theoretical background, and demonstrate a complete data analysis workflow including preprocessing, model training, evaluation, and a comparative study using different linkage methods and distance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction\n",
        "Hierarchical clustering is an unsupervised learning method that seeks to build a hierarchy of clusters without requiring the number of clusters to be specified beforehand (though it can be interpreted later from a dendrogram). We primarily distinguish between two approaches:\n",
        "- **Agglomerative (Bottom-Up) Clustering:** Start with each data point as its own cluster and iteratively merge clusters.\n",
        "- **Divisive (Top-Down) Clustering:** Start with one cluster containing all points and iteratively split it into smaller clusters.\n",
        "\n",
        "**Significance & Applications:**\n",
        "- **Marketing & Customer Segmentation:** Identify groups of similar customers to personalize marketing strategies.\n",
        "- **Genealogy & Evolutionary Studies:** Analyze common ancestors and build phylogenetic trees.\n",
        "- **Social Network Analysis:** Group similar users or communities.\n",
        "- **Text/Document Clustering:** Group similar documents to organize information.\n",
        "\n",
        "Hierarchical clustering is particularly helpful when we need to visualize and interpret the nested structure of data. The dendrograms offer insights into how clusters form at various distance thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Selection & Description\n",
        "For this project, we'll use **Mall_Customers.csv** from the [Datasets repository](https://github.com/MainakRepositor/Datasets). This dataset typically contains information on customers, including:\n",
        "- **CustomerID**: Unique identifier\n",
        "- **Genre**: Categorical feature\n",
        "- **Age**: Numeric feature\n",
        "- **Annual Income (k$)**: Numeric feature\n",
        "- **Spending Score (1-100)**: Numeric feature\n",
        "\n",
        "**Relevance for Hierarchical Clustering**\n",
        "This dataset is popular for clustering tasks (especially in marketing segmentation). Customers can be grouped based on similar traits like annual income and spending habits, allowing businesses to target them more effectively. Hierarchical Clustering helps derive an intuitive dendrogram that can reveal different cluster structures at various similarity thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "\n",
        "# Load the dataset\n",
        "data_path = 'https://raw.githubusercontent.com/MainakRepositor/Datasets/refs/heads/master/Mall_Customers.csv'\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploratory Data Analysis (EDA)\n",
        "We will:\n",
        "- Print statistical summaries of the dataset.\n",
        "- Visualize distributions and pairwise relationships.\n",
        "- Detect patterns and outliers in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numeric columns summary\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA visualizations\n",
        "plt.figure(figsize=(10,4))\n",
        "sns.countplot(x='Genre', data=df)\n",
        "plt.title('Genre Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92fe9c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histograms for numerical features\n",
        "numeric_cols = ['Age','Annual Income (k$)','Spending Score (1-100)']\n",
        "df[numeric_cols].hist(bins=10, figsize=(12,5))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5662cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pairplot\n",
        "sns.pairplot(df[numeric_cols])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc786b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='Blues')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preprocessing\n",
        "1. **Handling Missing Values:** Drop or impute missing values if they exist.\n",
        "2. **Encoding Categorical Variables:** Convert genre to numeric if needed.\n",
        "3. **Scaling/Normalizing Features:** Apply standard scaling or normalization to ensure equal influence of features in distance calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print('Missing Values in Each Column:\\n', missing_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "15faa482",
      "metadata": {},
      "outputs": [],
      "source": [
        "# For demonstration, assume minimal or no missing data; otherwise we'd address it.\n",
        "df.dropna(inplace=True)  # simple drop if minimal missing data\n",
        "\n",
        "# 2. Encoding categorical variables (if required)\n",
        "df['Genre'] = df['Genre'].map({'Male': 0, 'Female': 1})\n",
        "\n",
        "# 3. Selecting features relevant for clustering\n",
        "X = df[['Genre','Age','Annual Income (k$)','Spending Score (1-100)']].copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03962d7b",
      "metadata": {},
      "source": [
        "#### Note on Genre Encoding\n",
        "\n",
        "We encode the `Genre` column as `{'Male': 0, 'Female': 1}`. This encoding is arbitrary and does not imply any ordinal relationship between the categories. In unsupervised clustering, such encodings are often used to allow algorithms to process categorical data, but the numeric values themselves do not carry inherent meaning. The choice of encoding can influence clustering results, so it is important to interpret clusters with this in mind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d819e9ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "X_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58dd1578",
      "metadata": {},
      "source": [
        "#### Including Categorical Variables in Distance-Based Clustering\n",
        "\n",
        "In this analysis, we include the encoded `Genre` variable alongside numerical features when performing clustering. While this is a common approach, it is important to note that distance-based clustering algorithms (like hierarchical clustering) can be sensitive to the inclusion of categorical variables, especially when they are encoded as numbers. This can sometimes distort the distance calculations, as the algorithm may interpret the difference between categories as a meaningful numeric distance. In some cases, it may be preferable to exclude categorical variables or use specialized distance metrics that handle mixed data types. Here, we proceed with the inclusion for demonstration, but results should be interpreted with caution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mathematical Explanation\n",
        "**Agglomerative Clustering**\n",
        "- Start with each point as its own cluster.\n",
        "- At each iteration, merge the two \"closest\" clusters according to a linkage criterion.\n",
        "\n",
        "**Linkage Criteria**\n",
        "- **Single Linkage:** Distance between two clusters = minimum distance between any members.\n",
        "- **Complete Linkage:** Distance between two clusters = maximum distance between any members.\n",
        "- **Average Linkage:** Distance between two clusters = average pairwise distance between members.\n",
        "- **Ward's Method:** Minimizes the variance within each cluster.\n",
        "\n",
        "**Distance Metrics**\n",
        "- **Euclidean Distance:** $d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum (x_i - y_i)^2}$\n",
        "- **Manhattan Distance:** $d(\\mathbf{x}, \\mathbf{y}) = \\sum |x_i - y_i|$\n",
        "- Other metrics exist (e.g., Chebyshev, Minkowski) but Euclidean is most common.\n",
        "\n",
        "A dendrogram visualizes these merges. The vertical axis represents distance (or dissimilarity), and cutting the dendrogram at some level selects the desired number of clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1a17ca",
      "metadata": {},
      "source": [
        "#### Why Hierarchical Clustering for This Dataset?\n",
        "\n",
        "Hierarchical clustering is particularly well-suited for exploratory analysis when the number of clusters is not known in advance. The Mall Customers dataset is commonly used for customer segmentation, where the underlying group structure is not predefined. Hierarchical clustering provides a dendrogram, which visually represents how clusters are formed at different distance thresholds, allowing us to explore the data's natural grouping and select an appropriate number of clusters based on the data itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training & Evaluation\n",
        "We'll implement agglomerative hierarchical clustering with various linkage methods and distance metrics.\n",
        "\n",
        "Steps:\n",
        "1. Generate the linkage matrix using `scipy.cluster.hierarchy.linkage`.\n",
        "2. Plot the dendrogram to decide on the number of clusters.\n",
        "3. Use `fcluster` (or manually cut the dendrogram) to assign cluster labels.\n",
        "4. Evaluate the clusters using metrics like Silhouette Score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with Ward's linkage and Euclidean distance\n",
        "linked_ward = linkage(X_scaled, method='ward')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(linked_ward, truncate_mode='lastp', p=20)\n",
        "plt.title('Dendrogram (Ward Linkage)')\n",
        "plt.xlabel('Data Points')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96acaff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decide on a number of clusters by visually inspecting the dendrogram\n",
        "# Here, we assume ~5 clusters visually.\n",
        "labels_ward = fcluster(linked_ward, t=5, criterion='maxclust')\n",
        "\n",
        "print('Assigned Cluster Labels (Ward):', np.unique(labels_ward))\n",
        "\n",
        "# Evaluate with silhouette score\n",
        "score_ward = silhouette_score(X_scaled, labels_ward)\n",
        "print(f'Silhouette Score (Ward Linkage): {score_ward:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07302a6d",
      "metadata": {},
      "source": [
        "#### Understanding the Silhouette Score\n",
        "\n",
        "The silhouette score is a metric that quantifies how well each data point fits within its assigned cluster compared to other clusters. It ranges from -1 to 1:\n",
        "- A score close to 1 indicates that the data point is well-matched to its own cluster and poorly matched to neighboring clusters.\n",
        "- A score near 0 suggests that the data point is on or very close to the decision boundary between two clusters.\n",
        "- A negative score indicates that the data point may have been assigned to the wrong cluster.\n",
        "\n",
        "The average silhouette score across all data points provides an overall measure of clustering quality: higher values indicate better-defined clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a65748",
      "metadata": {},
      "source": [
        "#### 2D Visualization of Clusters\n",
        "\n",
        "To better visualize the clustering results, we use Principal Component Analysis (PCA) to project the data into two dimensions. Each point is colored according to its assigned cluster label. This helps us see the separation and cohesion of clusters in a reduced feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2b4c2a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "scatter = plt.scatter(X_pca[:,0], X_pca[:,1], c=labels_ward, cmap='tab10', s=50, alpha=0.7)\n",
        "plt.title('2D PCA Projection of Clusters (Ward Linkage)')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.colorbar(scatter, label='Cluster Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Analysis & Visualization\n",
        "In hierarchical clustering, **dendrograms** are crucial for understanding how clusters merge. We can also visualize the resulting clusters in various ways:\n",
        "- **Heatmaps** to observe cluster structure.\n",
        "- **Silhouette Coefficients** to check how well each data point fits into its assigned cluster.\n",
        "\n",
        "Below, we showcase an example cluster heatmap (by reorganizing the data according to cluster labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort data by cluster labels for a cluster heatmap\n",
        "df_analysis = X_scaled.copy()\n",
        "df_analysis['Cluster'] = labels_ward\n",
        "df_ordered = df_analysis.sort_values('Cluster')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_ordered.drop('Cluster', axis=1), cmap='viridis')\n",
        "plt.title('Heatmap of Scaled Features Ordered by Ward Cluster Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparative Study\n",
        "We will compare different linkage methods (Single, Complete, Average, Ward) and different distance metrics (e.g., Euclidean, Manhattan and cosine). Then we will compute the silhouette score for each approach.\n",
        "\n",
        "A simple approach:\n",
        "1. Iterate over linkages and metrics.\n",
        "2. Generate dendrograms (optionally) or directly compute cluster labels using a chosen cutoff.\n",
        "3. Compute silhouette scores for each combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linkage_methods = ['single','complete','average','ward']\n",
        "distance_metrics = ['euclidean','cityblock', 'cosine']  # cityblock is Manhattan\n",
        "results = []\n",
        "\n",
        "for method in linkage_methods:\n",
        "    for metric in distance_metrics:\n",
        "        # Ward linkage isn't compatible with metrics other than Euclidean.\n",
        "        # So if method='ward', skip if metric!='euclidean'\n",
        "        if method == 'ward' and metric != 'euclidean':\n",
        "            continue\n",
        "\n",
        "        Z = linkage(X_scaled, method=method, metric=metric)\n",
        "        # We'll pick the same cluster count (5) for consistency.\n",
        "        labels = fcluster(Z, t=5, criterion='maxclust')\n",
        "        score = silhouette_score(X_scaled, labels, metric=metric)\n",
        "        results.append((method, metric, score))\n",
        "\n",
        "print('Method | Metric | Silhouette Score')\n",
        "for r in results:\n",
        "    print(f'{r[0]:<10} | {r[1]:<9} | {r[2]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14306f75",
      "metadata": {},
      "source": [
        "#### Comparative Silhouette Scores\n",
        "\n",
        "The table and barplot below summarize the silhouette scores for different combinations of linkage methods and distance metrics. This visual comparison helps identify which clustering approach yields the most cohesive and well-separated clusters for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad71739",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Present results as DataFrame and plot\n",
        "results_df = pd.DataFrame(results, columns=['Linkage Method', 'Distance Metric', 'Silhouette Score'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(results_df)\n",
        "\n",
        "# Barplot for visual comparison\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    data=results_df,\n",
        "    x='Linkage Method',\n",
        "    y='Silhouette Score',\n",
        "    hue='Distance Metric'\n",
        ")\n",
        "plt.title('Comparative Silhouette Scores by Linkage and Distance Metric')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.xlabel('Linkage Method')\n",
        "plt.legend(title='Distance Metric')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion & Conclusion\n",
        "**Key Takeaways**\n",
        "- Different linkage methods can produce significantly different cluster structures. Average's method often performs well when Cosine distance is used.\n",
        "- The Silhouette Score provides a quantitative measure of how cohesive and separated the clusters are.\n",
        "- For this particular dataset, you might find that certain linkage methods (e.g., Average) with Cosine distance yield higher silhouette scores.\n",
        "\n",
        "**Potential Applications**\n",
        "- **Market Segmentation:** Identify customer groups for targeted marketing.\n",
        "- **Social Network Analysis:** Detect communities.\n",
        "- **Genealogy and Phylogenetics:** Understand ancestral relationships.\n",
        "\n",
        "**Limitations & Future Work**\n",
        "- **Scalability:** Hierarchical clustering can be computationally expensive for very large datasets.\n",
        "- **Distance Metric Sensitivity:** The choice of distance metric can drastically affect results.\n",
        "- **Automated Cluster Selection:** Deciding the number of clusters from a dendrogram can be subjective.\n",
        "- **Future**: Investigate advanced hierarchical methods, or combine hierarchical clustering with other clustering algorithms (e.g., K-means) for different stages of analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n",
        "- [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n",
        "- [Mall Customers Dataset](https://github.com/MainakRepositor/Datasets)\n",
        "- M. R. Anderberg, _Cluster Analysis for Applications_, Academic Press, 1973.\n",
        "- L. Kaufman and P. J. Rousseeuw, _Finding Groups in Data: An Introduction to Cluster Analysis_, Wiley, 1990."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
